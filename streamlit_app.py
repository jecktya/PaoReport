# -*- coding: utf-8 -*-

import streamlit as st
import requests
import urllib.parse
import html
from datetime import datetime, timedelta, timezone
import email.utils as eut
from bs4 import BeautifulSoup
import feedparser
from langdetect import detect

# API í‚¤ ë¡œë“œ
NAVER_CLIENT_ID = st.secrets.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = st.secrets.get("NAVER_CLIENT_SECRET")

# ì–¸ë¡ ì‚¬ ë§¤í•‘
press_name_map = {
    "chosun.com": "ì¡°ì„ ì¼ë³´", "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "hani.co.kr": "í•œê²¨ë ˆ",
    "joongang.co.kr": "ì¤‘ì•™ì¼ë³´", "mbn.co.kr": "MBN", "kbs.co.kr": "KBS",
    "sbs.co.kr": "SBS", "ytn.co.kr": "YTN", "donga.com": "ë™ì•„ì¼ë³´",
    "segye.com": "ì„¸ê³„ì¼ë³´", "munhwa.com": "ë¬¸í™”ì¼ë³´", "newsis.com": "ë‰´ì‹œìŠ¤",
    "naver.com": "ë„¤ì´ë²„", "daum.net": "ë‹¤ìŒ", "kukinews.com": "êµ­ë¯¼ì¼ë³´",
    "kookbang.dema.mil.kr": "êµ­ë°©ì¼ë³´", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "news1.kr": "ë‰´ìŠ¤1", "mbnmoney.mbn.co.kr": "MBN", "news.kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "jtbc.co.kr": "JTBC"
}

def extract_press_name(url):
    try:
        domain = urllib.parse.urlparse(url).netloc.replace("www.", "")
        # ì„œë¸Œë„ë©”ì¸ ì²˜ë¦¬: ë§¤í•‘ í‚¤ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í•´ë‹¹ í‚¤ë¡œ ëë‚˜ë©´ ë§¤í•‘
        for key, name in press_name_map.items():
            if domain == key or domain.endswith("." + key):
                return domain, name
        # ë§¤í•‘ë˜ì§€ ì•Šìœ¼ë©´ ë„ë©”ì¸ ìì²´ ë°˜í™˜
        return domain, domain
    except Exception:
        return None, None
    except:
        return None, None


def convert_to_mobile_link(url):
    if "n.news.naver.com/article" in url:
        return url.replace("n.news.naver.com/article", "n.news.naver.com/mnews/article")
    return url


def search_news(query):
    enc = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={enc}&display=30&sort=date"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    r = requests.get(url, headers=headers)
    if r.status_code == 200:
        return r.json().get("items", [])
    return []


def parse_pubdate(pubdate_str):
    try:
        dt = datetime(*eut.parsedate(pubdate_str)[:6], tzinfo=timezone(timedelta(hours=9)))
        return dt
    except:
        return None

# ì„¸ì…˜ ì´ˆê¸°í™”
if "final_articles" not in st.session_state:
    st.session_state.final_articles = []
if "selected_keys" not in st.session_state:
    st.session_state.selected_keys = []
if "copied_text" not in st.session_state:
    st.session_state.copied_text = ""

# UI: ì œëª© ë° ì˜µì…˜
st.title("ğŸ“° ë‰´ìŠ¤ê²€ìƒ‰ê¸°")
search_mode = st.radio("ğŸ—‚ï¸ ê²€ìƒ‰ ìœ í˜• ì„ íƒ", ["ì „ì²´", "ë™ì˜ìƒë§Œ", "ì£¼ìš”ì–¸ë¡ ì‚¬ë§Œ"])
st.markdown(
    f"<span style='color:gray;'>ğŸ•’ í˜„ì¬ ì‹œê°: {datetime.now(timezone(timedelta(hours=9))).strftime('%Y-%m-%d %H:%M:%S')} (4ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ ê²€ìƒ‰í•´ìš”)</span>",
    unsafe_allow_html=True
)

def_keywords = ["ìœ¡êµ°", "êµ­ë°©", "ì™¸êµ", "ì•ˆë³´", "ë¶í•œ",
                "ì‹ ë³‘êµìœ¡ëŒ€", "í›ˆë ¨", "ê°„ë¶€", "ì¥êµ",
                "ë¶€ì‚¬ê´€", "ë³‘ì‚¬", "ìš©ì‚¬", "êµ°ë¬´ì›"]
input_keywords = st.text_input("ğŸ” í‚¤ì›Œë“œ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", ", ".join(def_keywords))
keyword_list = [k.strip() for k in input_keywords.split(",") if k.strip()]

# ê²€ìƒ‰ ë²„íŠ¼
if st.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰"):
    with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
        now = datetime.now(timezone(timedelta(hours=9)))
        url_map = {}

        for kw in keyword_list:
            items = search_news(kw)
            for a in items:
                title = html.unescape(a["title"]).replace("<b>", "").replace("</b>", "")
                desc = html.unescape(a.get("description", "")).replace("<b>", "").replace("</b>", "")
                url = a["link"]
                pub = parse_pubdate(a.get("pubDate", "")) or datetime.min.replace(tzinfo=timezone(timedelta(hours=9)))
                domain, press = extract_press_name(a.get("originallink") or url)

                # 4ì‹œê°„ í•„í„°
                if not pub or (now - pub > timedelta(hours=4)):
                    continue

                # ëª¨ë“œë³„ í•„í„°
                if search_mode == "ì£¼ìš”ì–¸ë¡ ì‚¬ë§Œ" and press not in press_name_map.values():
                    continue
                if search_mode == "ë™ì˜ìƒë§Œ":
                    if press not in press_name_map.values():
                        continue
                    video_keys = ["ì˜ìƒ", "ë™ì˜ìƒ", "ì˜ìƒë³´ê¸°", "ë³´ëŸ¬ê°€ê¸°", "ë‰´ìŠ¤ì˜ìƒ", "ì˜ìƒë‰´ìŠ¤", "í´ë¦­í•˜ì„¸ìš”", "ë°”ë¡œë³´ê¸°"]
                    video_text = any(k in desc for k in video_keys) or any(k in title for k in video_keys)
                    video_url = any(p in url for p in ["/v/", "/video/", "vid="])
                    if not (video_text or video_url):
                        continue

                # ì¤‘ë³µ URL ê´€ë¦¬ ë° í‚¤ì›Œë“œ ë§¤í•‘
                if url not in url_map:
                    url_map[url] = {
                        "title": title,
                        "url": url,
                        "press": press,
                        "pubdate": pub,
                        "matched": set([kw])
                    }
                else:
                    url_map[url]["matched"].add(kw)

        # ê²°ê³¼ ì •ë¦¬
        articles = []
        for v in url_map.values():
            v["matched"] = sorted(v["matched"])
            articles.append(v)
        sorted_list = sorted(articles, key=lambda x: x['pubdate'], reverse=True)
        st.session_state.final_articles = sorted_list
        st.session_state.selected_keys = [a['url'] for a in sorted_list]

# ê²°ê³¼ ì¶œë ¥
if st.session_state.final_articles:
    st.subheader("ğŸ§¾ ê¸°ì‚¬ ë¯¸ë¦¬ë³´ê¸° ë° ë³µì‚¬")
    col1, _ = st.columns([0.3, 0.7])
    with col1:
        if st.button("âœ… ì „ì²´ ì„ íƒ"):
            st.session_state.selected_keys = [a['url'] for a in st.session_state.final_articles]
        if st.button("âŒ ì „ì²´ í•´ì œ"):
            st.session_state.selected_keys = []

    result_texts = []
    for art in st.session_state.final_articles:
        key = art['url']
        checked = key in st.session_state.selected_keys
        pub_str = art['pubdate'].strftime('%Y-%m-%d %H:%M')
        matched = ", ".join(art['matched'])

        st.markdown(
            f"<div style='user-select: text;'>â–  {art['title']} ({art['press']})</div>",
            unsafe_allow_html=True
        )
        st.markdown(
            f"<div style='color:gray;font-size:13px;'>ğŸ•’ {pub_str} | í‚¤ì›Œë“œ: {matched}</div>",
            unsafe_allow_html=True
        )
        new_check = st.checkbox("ì„ íƒ", value=checked, key=key)
        if new_check and key not in st.session_state.selected_keys:
            st.session_state.selected_keys.append(key)
        elif not new_check and key in st.session_state.selected_keys:
            st.session_state.selected_keys.remove(key)

        col_preview, col_copy = st.columns([0.75, 0.25])
        with col_preview:
            st.markdown(f"[ğŸ“ ê¸°ì‚¬ ë°”ë¡œë³´ê¸°]({convert_to_mobile_link(art['url'])})")
        with col_copy:
            if st.button("ğŸ“‹ 1ê±´ ë³µì‚¬", key=key + "_copy"):
                ctext = f"[{art['press']}] {art['title']}\n{convert_to_mobile_link(art['url'])}"
                st.session_state.copied_text = ctext

        if st.session_state.get("copied_text", "").startswith(f"[{art['press']}] {art['title']}"):
            st.text_area("ë³µì‚¬ëœ ë‚´ìš©", st.session_state.copied_text, height=80)

        if key in st.session_state.selected_keys:
            result_texts.append(f"â–  {art['title']} ({art['press']})\n{convert_to_mobile_link(art['url'])}")

    final_txt = "\n\n".join(result_texts)
    st.text_area("ğŸ“ ë³µì‚¬í•  ë‰´ìŠ¤ ëª©ë¡", final_txt, height=300)
    st.download_button("ğŸ“„ ë³µì‚¬ ë‚´ìš© ë‹¤ìš´ë¡œë“œ (.txt)", final_txt, file_name="news.txt")
    st.markdown("ğŸ“‹ ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë³µì‚¬í•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì €ì¥í•˜ì„¸ìš”.")

